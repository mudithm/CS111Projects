NAME: Mudith Mallajosyula
EMAIL: mudithm@g.ucla.edu
ID: 404937201

SLIPDAYS: 5

----------------------------------------------------

Included Files:

lab2b_list.c -- the main C source file for the program.

		It uses the getopts library for interpreting command-line options.

SortedList.c -- implementation of a doubly-linked list of string keys

SortedList.h -- header file for a doubly-linked list of string keys

lab2b_list.gp -- gnuplot program to create list graphs

Various graphs created by the above program

Makefile -- for creating the executable, the tarball, graphs, and performing tests


The Makefile include the following targets:

	build: the default target, which creates the executables (lab2_add,
	lab2_list).
	clean: removes all programs created by the Makefile
	dist: creates the tarball consisting of the source file, Makefile, 
			graphs, data, profile, and the README.
	test: Contains tests for the list program
	profile: profiles the list program with 1000 iterations and spin-lock,
	to see which parts of the program take the most time. Produces profile.out.
	
Data -- lab2b_list.csv, created by the tests target

---------------------------------------------------------


Sources:
	
	example of getopt_long:
	https://www.gnu.org/software/libc/manual/html_node/Getopt-Long-Option-Example.html
	
	multiline bash commands in the makefile:
	https://stackoverflow.com/questions/10121182/multiline-bash-commands-in-makefile

	strstr c function:
	http://www.cplusplus.com/reference/cstring/strstr/

	List of linux kill signals:
	https://unix.stackexchange.com/questions/317492/list-of-kill-signals

	djb2 hash:
	http://www.cse.yorku.ca/~oz/hash.html

	google-pprof:
	https://gperftools.github.io/gperftools/cpuprofile.html

-------------------------------------------------------------

Limitations:

	Although this is expected, the list functions often fail for high thread
	counts without protection

------------------------------------------------------------

======================
QUESTIONS
======================

2.3.1
-----

For the 1 and 2 thread lists, most of the time is spent performing the
SortedList operations, such as insertion, deletion, and length checking.

This is because, for low thread counts, there is not much contention for access
to the lists, since the likelihood of two threads accessing the list
simultaneously is low. So, rather than spending large amounts of time waiting
for access to a critical section, the threads act independently for the majority
of the time.

For the high-thread spin-lock tests, most of the time is spent waiting on the
spin-locks, since spin-locks perform busy waits that do not allow the processor
to perform more useful tasks while waiting on a critical section.

For the high-thread mutex tests, most of the time is spent performing the list
operations, as with the low-thread tests. This is because mutex locks use
blocking, and allow the processor to perform more useful tasks while waiting on
the locks.


2.3.2
-----

Most of the CPU time in the spin-lock version of the list exerciser for large
thread count is spent waiting for access to critical sections. From the
profiler, these sections are:

   586    586  287:         while (__sync_lock_test_and_set(&(list->spin_lock), 1))
     4      4  288:             continue;


   541    541  354:         while (__sync_lock_test_and_set(&(list->spin_lock), 1))
     1      1  355:             continue;

This operation is so expensive for large thread counts because, with more
threads trying to access the same lists, more time must be spent by each waiting
for the critical section to available, as there is more contention. Since only
one thread at a time can access each list, and all other threads have to wait to
access the list, more time is spent waiting.


2.3.3
-----

The average lock-wait time rises so dramatically with the number of contending
threads because increased contention for access to critical sections means that
more time must be spent by each thread waiting. Although mutex locks allow
blocking, if the processor does not have any other tasks to perform, it still
has to wait for access to the critical section.

The completion time per operation rises less dramatically with the number of
contending threads because it factors in the time spent in contention and the
time for each context switch, which increases with more threads.

The wait time can go up faster than the mean completion time because the wait
time for different threads can overlap. For this reason, the wait time rises
much faster than the completion time.


2.3.4
-----

The performance of the synchronized methods improves as the number of lists
increases. This occurs for several reasons. Since there are more lists to store
the same amount of information, there is a lower chance of contention in each
list, resulting in shorter average wait times. Additionally, each sublist is
shorter than the original list, and so operations like inserting and lookups
will run faster.

The throughput should continue to increase as the number of lists increases,
until the benefit of having lower conention per list is outweighed by the higher
cost of initializing, finding the length of, and deleting from lists. If there
are too many lists, very few elements will be contained in each list, if any,
and balancing becomes an issue.

It does not appear to be true from the curves; partitioning the list has the
added benefits of making length-dependent operations faster, thus reducing the
average time spend inside the critical section. This decreases the chances of
contention more than simply having fewer threads contending for a single list.